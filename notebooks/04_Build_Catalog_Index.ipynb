{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ecf60e2-a6b4-4577-ab42-30d25a8d52ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from faiss-cpu) (22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ef88fd-0731-477f-a8dd-9733c6840672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "from ultralytics import YOLO\n",
    "import faiss\n",
    "import json\n",
    "from tqdm.notebook import tqdm # For a nice progress bar!\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# Device setup for M1 Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91446ea4-624f-42a0-9e4e-fcf333253710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8 model loaded.\n",
      "CLIP model loaded.\n"
     ]
    }
   ],
   "source": [
    "# This cell is the same as before\n",
    "import ssl\n",
    "import certifi\n",
    "import urllib.request\n",
    "import clip\n",
    "\n",
    "# Use certifi's certificate bundle\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "urllib.request.install_opener(\n",
    "    urllib.request.build_opener(\n",
    "        urllib.request.HTTPSHandler(context=ssl_context)\n",
    "    )\n",
    ")\n",
    "# Load YOLO\n",
    "YOLO_MODEL_PATH = '../models/best.pt'\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device)\n",
    "print(\"YOLOv8 model loaded.\")\n",
    "\n",
    "# Load CLIP\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "print(\"CLIP model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0988f33-8814-4635-b2e8-fe231f6ed0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded catalog with 9172 images.\n",
      "We will process a sample of 9172 images for this test run.\n"
     ]
    }
   ],
   "source": [
    "CATALOG_PATH = '../data/catalog_full.csv'\n",
    "df_catalog = pd.read_csv(CATALOG_PATH)\n",
    "\n",
    "# For testing, you might not want to run all 11k images. \n",
    "# Let's create a smaller sample for development.\n",
    "# When you're ready for the full run, comment out the line below.\n",
    "\n",
    "print(f\"Loaded catalog with {len(df_catalog)} images.\")\n",
    "print(f\"We will process a sample of {len(df_catalog)} images for this test run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1116bc5-4623-48fd-85bd-e9ea658f4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (8.1.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: stack_data in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: decorator in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/ravinder/Desktop/flickd-hackathon/venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/ravinder/Desktop/flickd-hackathon/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eed68fd-9061-473d-984f-5b1e76f90940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5b968ac16b4ceaaeb19a471e10b2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images:   0%|          | 0/9172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete.\n",
      "Successfully generated 7922 embeddings.\n",
      "Shape of the final matrix: (7922, 768)\n"
     ]
    }
   ],
   "source": [
    "# These lists will store our results\n",
    "all_embeddings = []\n",
    "product_id_map = [] # This will map the index of an embedding to its product ID\n",
    "\n",
    "# Use tqdm to wrap our dataframe iterator for a progress bar\n",
    "for index, row in tqdm(df_catalog.iterrows(), total=df_catalog.shape[0], desc=\"Processing Images\"):\n",
    "    image_url = row['image_url']\n",
    "    product_id = row['id']\n",
    "\n",
    "    try:\n",
    "        # 1. Download image\n",
    "        response = requests.get(image_url, stream=True, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(response.raw).convert(\"RGB\")\n",
    "\n",
    "        # 2. Detect with YOLO\n",
    "        results = yolo_model(image, verbose=False)\n",
    "\n",
    "        # 3. Find the best box (the one with the largest area)\n",
    "        best_box = None\n",
    "        max_area = 0\n",
    "        if len(results[0].boxes) > 0:\n",
    "            for box in results[0].boxes:\n",
    "                # Calculate the area of the box\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                # If this box is bigger than the biggest one we've seen so far, it's our new best\n",
    "                if area > max_area:\n",
    "                    max_area = area\n",
    "                    best_box = box\n",
    "        \n",
    "        # 4. If a best box was found, crop and get embedding\n",
    "        if best_box is not None:\n",
    "            x1, y1, x2, y2 = map(int, best_box.xyxy[0])\n",
    "            cropped_image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "            # 5. Generate CLIP embedding\n",
    "            image_input = preprocess(cropped_image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.encode_image(image_input)\n",
    "            \n",
    "            # Normalize and convert to a CPU numpy array for FAISS\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            embedding_np = image_features.cpu().numpy()\n",
    "\n",
    "            # 6. Store the results\n",
    "            all_embeddings.append(embedding_np)\n",
    "            product_id_map.append(int(product_id))\n",
    "\n",
    "    except Exception as e:\n",
    "        # This will catch download errors, processing errors, etc.\n",
    "        # print(f\"Skipping image at index {index} due to error: {e}\")\n",
    "        pass # We just ignore errors for this batch job\n",
    "\n",
    "# Convert the list of embeddings into a single large NumPy array\n",
    "if all_embeddings:\n",
    "    embeddings_matrix = np.vstack(all_embeddings)\n",
    "    print(f\"\\nProcessing complete.\")\n",
    "    print(f\"Successfully generated {embeddings_matrix.shape[0]} embeddings.\")\n",
    "    print(f\"Shape of the final matrix: {embeddings_matrix.shape}\")\n",
    "else:\n",
    "    print(\"\\nProcessing complete. No embeddings were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e810a81-2b1f-4d1d-a5a0-d13f3d98fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAISS index created.\n",
      "Total embeddings in index: 7922\n",
      "FAISS index saved to: ../models/catalog_index_large.faiss\n",
      "Product ID map saved to: ../models/product_id_map_large.json\n"
     ]
    }
   ],
   "source": [
    "if 'embeddings_matrix' in locals() and embeddings_matrix.shape[0] > 0:\n",
    "    # Get the dimension of our embeddings (should be 512)\n",
    "    d = embeddings_matrix.shape[1]\n",
    "\n",
    "    # Create a FAISS index. IndexFlatL2 is a standard choice for dense vectors.\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Add our embeddings matrix to the index\n",
    "    index.add(embeddings_matrix)\n",
    "\n",
    "    print(f\"\\nFAISS index created.\")\n",
    "    print(f\"Total embeddings in index: {index.ntotal}\")\n",
    "\n",
    "    # --- Save the files ---\n",
    "    OUTPUT_DIR = '../models/'\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # 1. Save the FAISS index\n",
    "    faiss.write_index(index, os.path.join(OUTPUT_DIR, \"catalog_index_large.faiss\"))\n",
    "    print(f\"FAISS index saved to: {os.path.join(OUTPUT_DIR, 'catalog_index_large.faiss')}\")\n",
    "\n",
    "    # 2. Save the product ID mapping\n",
    "    with open(os.path.join(OUTPUT_DIR, \"product_id_map_large.json\"), 'w') as f:\n",
    "        json.dump(product_id_map, f)\n",
    "    print(f\"Product ID map saved to: {os.path.join(OUTPUT_DIR, 'product_id_map_large.json')}\")\n",
    "\n",
    "else:\n",
    "    print(\"No embeddings were generated, skipping index creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab0dbb0-54ac-4c90-a694-cb97574286d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the embeddings matrix is: (7922, 768)\n"
     ]
    }
   ],
   "source": [
    "# In a new cell at the end of your \"build large index\" notebook\n",
    "if 'embeddings_matrix' in locals():\n",
    "    print(f\"The shape of the embeddings matrix is: {embeddings_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbeedc1-f22f-4672-9657-54b3858dc6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flickd-hackathon",
   "language": "python",
   "name": "flickd-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
