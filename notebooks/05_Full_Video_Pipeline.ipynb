{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b8c530-991c-4186-90a2-fc8f4cb0e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "from ultralytics import YOLO\n",
    "import faiss\n",
    "import json\n",
    "import cv2  # For video processing\n",
    "from tqdm.notebook import tqdm\n",
    "import collections\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf36a147-f071-48f3-b0ca-d3f8c7c55f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO and CLIP models loaded.\n",
      "FAISS index loaded with 7922 vectors.\n",
      "Master catalog data loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Paths ---\n",
    "MODELS_DIR = '../models/'\n",
    "DATA_DIR = '../data/'\n",
    "VIDEO_DIR = '../videos/'\n",
    "\n",
    "# --- Load Models (YOLO and CLIP) ---\n",
    "yolo_model = YOLO(os.path.join(MODELS_DIR, 'best.pt')).to(device)\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(\"YOLO and CLIP models loaded.\")\n",
    "\n",
    "# --- Load FAISS Index and Mapping ---\n",
    "index = faiss.read_index(os.path.join(MODELS_DIR, \"catalog_index.faiss\"))\n",
    "with open(os.path.join(MODELS_DIR, \"product_id_map.json\"), 'r') as f:\n",
    "    product_id_map = json.load(f)\n",
    "print(f\"FAISS index loaded with {index.ntotal} vectors.\")\n",
    "\n",
    "# --- Load Master Catalog to get Product Details ---\n",
    "df_catalog = pd.read_csv(os.path.join(DATA_DIR, 'catalog_full.csv')).set_index('id')\n",
    "print(\"Master catalog data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e7719e-f6bc-4918-8924-685982fa68fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing function is defined.\n"
     ]
    }
   ],
   "source": [
    "def process_video_for_products(video_path, yolo_model, clip_model, preprocess, faiss_index, id_map, device, frame_rate=1):\n",
    "    \"\"\"\n",
    "    Processes a video to find matching products from the catalog.\n",
    "    \n",
    "    :param frame_rate: How many frames per second to process.\n",
    "    :return: A list of detected product IDs and their similarity scores.\n",
    "    \"\"\"\n",
    "    detected_products = []\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(fps / frame_rate)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        success, frame_bgr = vidcap.read()\n",
    "        if not success:\n",
    "            break # End of video\n",
    "        \n",
    "        # Process only at the desired frame rate\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert frame from BGR (OpenCV) to RGB (Pillow/CLIP)\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # 1. Detect with YOLO\n",
    "            results = yolo_model(image, verbose=False)\n",
    "\n",
    "            # 2. Find the best box\n",
    "            best_box = None\n",
    "            max_area = 0\n",
    "            if len(results[0].boxes) > 0:\n",
    "                for box in results[0].boxes:\n",
    "                    area = (box.xyxy[0][2] - box.xyxy[0][0]) * (box.xyxy[0][3] - box.xyxy[0][1])\n",
    "                    if area > max_area:\n",
    "                        max_area = area\n",
    "                        best_box = box\n",
    "\n",
    "            # 3. If a box is found, get embedding and search\n",
    "            if best_box is not None:\n",
    "                x1, y1, x2, y2 = map(int, best_box.xyxy[0])\n",
    "                cropped_image = image.crop((x1, y1, x2, y2))\n",
    "                \n",
    "                # Get CLIP embedding\n",
    "                image_input = preprocess(cropped_image).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    image_features = clip_model.encode_image(image_input)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                embedding_np = image_features.cpu().numpy()\n",
    "\n",
    "                # 4. Search FAISS\n",
    "                k = 5 # Number of nearest neighbors to find\n",
    "                distances, indices = faiss_index.search(embedding_np, k)\n",
    "                \n",
    "                # Store results\n",
    "                for i in range(k):\n",
    "                    match_index = indices[0][i]\n",
    "                    match_distance = distances[0][i]\n",
    "                    # Convert L2 distance to a pseudo-similarity score (0-1)\n",
    "                    similarity = 1 / (1 + match_distance) \n",
    "                    \n",
    "                    product_id = id_map[match_index]\n",
    "                    detected_products.append({'id': product_id, 'similarity': similarity})\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "    vidcap.release()\n",
    "    return detected_products\n",
    "\n",
    "print(\"Video processing function is defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17137e7d-8f7d-4bc8-ad0e-22517e999db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def process_video_in_batches(video_path, yolo_model, clip_model, preprocess, faiss_index, id_map, device, frame_rate=1, batch_size=10):\n",
    "    \"\"\"\n",
    "    Processes a video in small batches to prevent memory-related kernel crashes.\n",
    "    \n",
    "    :param frame_rate: How many frames per second to sample from the video.\n",
    "    :param batch_size: How many frames to process at a time before clearing memory.\n",
    "    :return: A list of detected product IDs and their similarity scores.\n",
    "    \"\"\"\n",
    "    detected_products = []\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        print(\"Warning: Could not determine video FPS. Assuming 30.\")\n",
    "        fps = 30\n",
    "        \n",
    "    frame_interval = int(fps / frame_rate)\n",
    "    if frame_interval == 0: frame_interval = 1\n",
    "    \n",
    "    frame_count = 0\n",
    "    frame_buffer = [] # This will hold the frames for one batch\n",
    "\n",
    "    while True:\n",
    "        success, frame_bgr = vidcap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame_rgb)\n",
    "            frame_buffer.append(image)\n",
    "\n",
    "            # WHEN THE BATCH IS FULL, PROCESS IT\n",
    "            if len(frame_buffer) >= batch_size:\n",
    "                print(f\"--- Processing batch of {len(frame_buffer)} frames ---\")\n",
    "                \n",
    "                # YOLO can process a list of images at once - this is very efficient\n",
    "                results_batch = yolo_model(frame_buffer, verbose=False)\n",
    "\n",
    "                # Now loop through the results FOR THIS BATCH\n",
    "                for i, results in enumerate(results_batch):\n",
    "                    original_image = frame_buffer[i]\n",
    "                    \n",
    "                    # Same 'best box' logic as before\n",
    "                    best_box = None\n",
    "                    max_area = 0\n",
    "                    if len(results.boxes) > 0:\n",
    "                        for box in results.boxes:\n",
    "                            area = (box.xyxy[0][2] - box.xyxy[0][0]) * (box.xyxy[0][3] - box.xyxy[0][1])\n",
    "                            if area > max_area:\n",
    "                                max_area = area\n",
    "                                best_box = box\n",
    "                    \n",
    "                    # If a box is found, get embedding and search\n",
    "                    if best_box is not None:\n",
    "                        x1, y1, x2, y2 = map(int, best_box.xyxy[0])\n",
    "                        cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "                        \n",
    "                        image_input = preprocess(cropped_image).unsqueeze(0).to(device)\n",
    "                        with torch.no_grad():\n",
    "                            image_features = clip_model.encode_image(image_input)\n",
    "                        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                        embedding_np = image_features.cpu().numpy()\n",
    "\n",
    "                        k = 5\n",
    "                        distances, indices = faiss_index.search(embedding_np, k)\n",
    "                        \n",
    "                        for j in range(k):\n",
    "                            similarity = 1 / (1 + distances[0][j])\n",
    "                            product_id = id_map[indices[0][j]]\n",
    "                            detected_products.append({'id': product_id, 'similarity': similarity})\n",
    "\n",
    "                # --- MEMORY CLEANUP (THE CRITICAL PART) ---\n",
    "                frame_buffer.clear() # Empty the buffer\n",
    "                del results_batch # Explicitly delete the large results object\n",
    "                gc.collect() # Force Python's garbage collector to run\n",
    "                print(\"--- Batch processed and memory cleared ---\")\n",
    "\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Process any leftover frames in the buffer after the loop finishes\n",
    "    if frame_buffer:\n",
    "        print(f\"--- Processing final batch of {len(frame_buffer)} frames ---\")\n",
    "        # (Repeat the same processing logic as inside the loop)\n",
    "        results_batch = yolo_model(frame_buffer, verbose=False)\n",
    "        for i, results in enumerate(results_batch):\n",
    "            original_image = frame_buffer[i]\n",
    "            best_box = None; max_area = 0\n",
    "            if len(results.boxes) > 0:\n",
    "                for box in results.boxes:\n",
    "                    area = (box.xyxy[0][2] - box.xyxy[0][0]) * (box.xyxy[0][3] - box.xyxy[0][1])\n",
    "                    if area > max_area:\n",
    "                        max_area = area; best_box = box\n",
    "            if best_box is not None:\n",
    "                x1, y1, x2, y2 = map(int, best_box.xyxy[0]); cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "                image_input = preprocess(cropped_image).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    image_features = clip_model.encode_image(image_input)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True); embedding_np = image_features.cpu().numpy()\n",
    "                k = 5; distances, indices = faiss_index.search(embedding_np, k)\n",
    "                for j in range(k):\n",
    "                    similarity = 1 / (1 + distances[0][j]); product_id = id_map[indices[0][j]]\n",
    "                    detected_products.append({'id': product_id, 'similarity': similarity})\n",
    "    \n",
    "    vidcap.release()\n",
    "    print(\"Video processing complete.\")\n",
    "    return detected_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83538233-964c-4f04-b9b6-4eeb21a58ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING YOLO-ONLY DEBUG RUN ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6339cdbe51f44391bcfa5755a9bc9ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "YOLO Debug Run:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- YOLO DEBUG COMPLETE ---\n",
      "Processed video without crashing. Total boxes found: 32\n"
     ]
    }
   ],
   "source": [
    "# --- Choose a video to test ---\n",
    "\n",
    "test_video_name = '2025-05-28_13-42-32_UTC.mp4' \n",
    "\n",
    "\n",
    "test_video_path = os.path.join(VIDEO_DIR, test_video_name)\n",
    "\n",
    "if os.path.exists(test_video_path):\n",
    "    print(f\"Processing video: {test_video_name} with batching...\")\n",
    "    \n",
    "    # --- Calling the NEW function ---\n",
    "    all_detections = process_video_in_batches(\n",
    "        video_path=test_video_path, \n",
    "        yolo_model=yolo_model, \n",
    "        clip_model=clip_model, \n",
    "        preprocess=preprocess, \n",
    "        faiss_index=index, \n",
    "        id_map=product_id_map, \n",
    "        device=device,\n",
    "        frame_rate=1,  # Process 2 frames per second\n",
    "        batch_size=1  # Process them in chunks of 10\n",
    "    )\n",
    "    # -------------------------------\n",
    "\n",
    "    print(f\"Finished processing. Found {len(all_detections)} potential matches across all frames.\")\n",
    "else:\n",
    "    print(f\"Video file not found at {test_video_path}. Please check the file name and location.\")\n",
    "    all_detections = []\n",
    "\n",
    "# --- The rest of the cell stays the same ---\n",
    "if all_detections:\n",
    "    df_results = pd.DataFrame(all_detections)\n",
    "    display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60462c-01c1-4fd2-8283-b46eee2c7475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flickd-hackathon",
   "language": "python",
   "name": "flickd-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
